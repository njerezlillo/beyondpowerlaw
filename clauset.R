###############################################################################
#
# library
#
library(VGAM)   # zeta function
library(R.matlab) # read matlab file just for test purpose
#
###############################################################################
#
# test zone
#
###############################################################################
runtest <- function(){
  #read a matlab file
  #x generated by x = randht(10000,'xmin',15,'powerlaw',2.5) with matlab (continuous law)
  x<-readMat(file("x_2.5_15_10000.mat","rb"))$x[,1]
  #plfit matlab:alpha=2.4975,xmin=18.5752,D=0.0062
  plfit(x)
  #plfit R:alpha=2.497485,xmin=18.57524,D=0.006174977
  #x0 generated by x0 =floor(x) with matlab (big discrete approximation)
  x0<-readMat(file("x0_2.5_15_10000.mat","rb"))$x[,1]
  #plfit matlab:alpha=2.4700,xmin=15,D=0.0056
  plfit(x0)
  #plfit r:alpha=2.47,xmin=15,D=0.00564463


}
###############################################################################
#
# PLFIT fits a power-law distributional model to data.
#
#    PLFIT(x) estimates x_min and alpha according to the goodness-of-fit
#    based method described in Clauset, Shalizi, Newman (2007). x is a
#    vector of observations of some quantity to which we wish to fit the
#    power-law distribution p(x) ~ x^-alpha for x >= xmin.
#    PLFIT automatically detects whether x is composed of real or integer
#    values, and applies the appropriate method. For discrete data, if
#    min(x) > 1000, PLFIT uses the continuous approximation, which is
#    a reliable in this regime.
#
#    The fitting procedure works as follows:
#    1) For each possible choice of x_min, we estimate alpha via the
#       method of maximum likelihood, and calculate the Kolmogorov-Smirnov
#       goodness-of-fit statistic D.
#    2) We then select as our estimate of x_min, the value that gives the
#       minimum value D over all values of x_min.
#
#    Note that this procedure gives no estimate of the uncertainty of the
#    fitted parameters, nor of the validity of the fit.
#
#    Example:
#       x <- (1-runif(10000))^(-1/(2.5-1))
#       plfit(x)
#
#
# Version 1.0   (2008 February)
# Version 1.1   (2008 February)
#    - correction : division by zero if limit >= max(x) because the unique R function do no sort
#                   and the matlab function do...
# Version 1.1   (minor correction 2009 August)
#    - correction : lines 230 zdiff calcul was wrong when xmin=0 (thanks to Naoki Masuda)
#    - gpl version updated to v3.0 (asked by Felipe Ortega)
# Version 1.2 (2011 August)
#    - correction for method "limit" thanks to David R. Pugh
#      xmins <- xmins[xmins<=limit] is now xmins <- xmins[xmins>=limit]
#    - "fixed" method added for xmins from David R. Pugh
#    - modifications by Alan Di Vittorio:
#       - correction : zdiff calculation was wrong when xmin==1
#       - the previous zdiff correction was incorrect
#       - correction : x has to have at least two unique values
#       - additional discrete x input test : discrete x cannot contain the value 0
#       - added option to truncate continuous xmin search when # of obs gets small
#
# Copyright (C) 2008,2011 Laurent Dubroca laurent.dubroca_at_gmail.com
# (Stazione Zoologica Anton Dohrn, Napoli, Italy)
# Distributed under GPL 3.0
# http://www.gnu.org/copyleft/gpl.html
# PLFIT comes with ABSOLUTELY NO WARRANTY
# Matlab to R translation based on the original code of Aaron Clauset (Santa Fe Institute)
# Source: http://www.santafe.edu/~aaronc/powerlaws/
#
# Notes:
#
# 1. In order to implement the integer-based methods in Matlab, the numeric
#    maximization of the log-likelihood function was used. This requires
#    that we specify the range of scaling parameters considered. We set
#    this range to be seq(1.5,3.5,0.01) by default. This vector can be
#    set by the user like so,
#
#       a <- plfit(x,"range",seq(1.001,5,0.001))
#
# 2. PLFIT can be told to limit the range of values considered as estimates
#    for xmin in two ways. First, it can be instructed to sample these
#    possible values like so,
#
#       a <- plfit(x,"sample",100)
#
#    which uses 100 uniformly distributed values on the sorted list of
#    unique values in the data set. Alternatively, it can simply omit all
#    candidates below a hard limit, like so
#
#       a <- plfit(x,"limit",3.4)
#
#    In the case of discrete data, it rounds the limit to the nearest
#    integer.
#
#     Finally, if you wish to force the threshold parameter to take a specific value
#     (useful for bootstrapping), simply call plfit() like so
#
#       a <- plfit(x,"fixed",3.5)
#
# 3. When the input sample size is small (e.g., < 100), the estimator is
#    known to be slightly biased (toward larger values of alpha). To
#    explicitly use an experimental finite-size correction, call PLFIT like
#    so
#
#       a <- plfit(x,finite=TRUE)
#
# 4. For continuous data, PLFIT can return erroneously large estimates of
#    alpha when xmin is so large that the number of obs x >= xmin is very
#    small. To prevent this, we can truncate the search over xmin values
#    before the finite-size bias becomes significant by calling PLFIT as
#
#       a = plfit(x,nosmall=TRUE);
#
#    which skips values xmin with finite size bias > 0.1.
#
###############################################################################
plfit<-function(x=rpareto(1000,10,2.5),method="limit",value=c(),finite=FALSE,nowarn=FALSE,nosmall=FALSE){
  #init method value to NULL
  vec <- c() ; sampl <- c() ; limit <- c(); fixed <- c()
  ###########################################################################################
  #
  #  test and trap for bad input
  #
  switch(method,
         range = vec <- value,
         sample = sampl <- value,
         limit = limit <- value,
         fixed = fixed <- value,
         argok <- 0)

  if(exists("argok")){stop("(plfit) Unrecognized method")}

  if( !is.null(vec) && (!is.vector(vec) || min(vec)<=1 || length(vec)<=1) ){
    print(paste("(plfit) Error: ''range'' argument must contain a vector > 1; using default."))
    vec <- c()
  }
  if( !is.null(sampl) && ( !(sampl==floor(sampl)) ||  length(sampl)>1 || sampl<2 ) ){
    print(paste("(plfit) Error: ''sample'' argument must be a positive integer > 2; using default."))
    sample <- c()
  }
  if( !is.null(limit) && (length(limit)>1 || limit<1) ){
    print(paste("(plfit) Error: ''limit'' argument must be a positive >=1; using default."))
    limit <- c()
  }
  if( !is.null(fixed) && (length(fixed)>1 || fixed<=0) ){
    print(paste("(plfit) Error: ''fixed'' argument must be a positive >0; using default."))
    fixed <- c()
  }

  #  select method (discrete or continuous) for fitting and test if x is a vector
  fdattype<-"unknow"
  if( is.vector(x,"numeric") ){ fdattype<-"real" }
  if( all(x==floor(x)) && is.vector(x) ){ fdattype<-"integer" }
  if( all(x==floor(x)) && min(x) > 1000 && length(x) > 100 ){ fdattype <- "real" }
  if( fdattype=="unknow" ){ stop("(plfit) Error: x must contain only reals or only integers.") }

  #
  #  end test and trap for bad input
  #
  ###########################################################################################

  ###########################################################################################
  #
  #  estimate xmin and alpha in the continuous case
  #
  if( fdattype=="real" ){

    xmins <- sort(unique(x))
    xmins <- xmins[-length(xmins)]

    if( !is.null(limit) ){
      xmins <- xmins[xmins>=limit]
    }
    if( !is.null(fixed) ){
      xmins <- fixed
    }
    if( !is.null(sampl) ){
      xmins <- xmins[unique(round(seq(1,length(xmins),length.out=sampl)))]
    }

    dat <- rep(0,length(xmins))
    z   <- sort(x)

    for( xm in 1:length(xmins) ){
      xmin <- xmins[xm]
      z    <- z[z>=xmin]
      n    <- length(z)
      # estimate alpha using direct MLE
      a    <- n/sum(log(z/xmin))
      # truncate search if nosmall is selected
      if( nosmall ){
        if((a-1)/sqrt(n) > 0.1){
          dat <- dat[1:(xm-1)]
          print(paste("(plfit) Warning : xmin search truncated beyond",xmins[xm-1]))
          break
        }
      }
      # compute KS statistic
      cx   <- c(0:(n-1))/n
      cf   <- 1-(xmin/z)^a
      dat[xm] <- max(abs(cf-cx))
    }

    D     <- min(dat)
    xmin  <- xmins[min(which(dat<=D))]
    z     <- x[x>=xmin]
    n     <- length(z)
    alpha <- 1 + n/sum(log(z/xmin))

    if( finite ){
      alpha <- alpha*(n-1)/n+1/n # finite-size correction
    }
    if( n<50 && !finite && !nowarn){
      print("(plfit) Warning : finite-size bias may be present")
    }

  }
  #
  #  end continuous case
  #
  ###########################################################################################

  ###########################################################################################
  #
  #  estimate xmin and alpha in the discrete case
  #
  if( fdattype=="integer" ){

    if( is.null(vec) ){ vec<-seq(1.1, 6, .01) } # seq(1.5,3.5,.01) covers range of most practical scaling parameters
    zvec <- zeta(vec)

    xmins <- sort(unique(x))
    xmins <- xmins[-length(xmins)]

    if( !is.null(limit) ){
      limit <- round(limit)
      xmins <- xmins[xmins>=limit]
    }

    if( !is.null(fixed) ){
      xmins <- fixed
    }

    if( !is.null(sampl) ){
      xmins <- xmins[unique(round(seq(1,length(xmins),length.out=sampl)))]
    }

    if( is.null(xmins) || length(xmins) < 2){
      stop("(plfit) error: x must contain at least two unique values.")
    }

    if(length(which(xmins==0) > 0)){
      stop("(plfit) error: x must not contain the value 0.")
    }

    xmax <- max(x)
    dat <- matrix(0,nrow=length(xmins),ncol=2)
    z <- x
    for( xm in 1:length(xmins) ){
      xmin <- xmins[xm]
      z    <- z[z>=xmin]
      n    <- length(z)
      # estimate alpha via direct maximization of likelihood function
      #  vectorized version of numerical calculation
      # matlab: zdiff = sum( repmat((1:xmin-1)',1,length(vec)).^-repmat(vec,xmin-1,1) ,1);
      if(xmin==1){
        zdiff <- rep(0,length(vec))
      }else{
        zdiff <- apply(rep(t(1:(xmin-1)),length(vec))^-t(kronecker(t(array(1,xmin-1)),vec)),2,sum)
      }
      # matlab: L = -vec.*sum(log(z)) - n.*log(zvec - zdiff);
      L <- -vec*sum(log(z)) - n*log(zvec - zdiff);
      I <- which.max(L)
      # compute KS statistic
      fit <- cumsum((((xmin:xmax)^-vec[I])) / (zvec[I] - sum((1:(xmin-1))^-vec[I])))
      cdi <- cumsum(hist(z,c(min(z)-1,(xmin+.5):xmax,max(z)+1),plot=FALSE)$counts/n)
      dat[xm,] <- c(max(abs( fit - cdi )),vec[I])
    }
    D     <- min(dat[,1])
    I     <- which.min(dat[,1])
    xmin  <- xmins[I]
    n     <- sum(x>=xmin)
    alpha <- dat[I,2]

    if( finite ){
      alpha <- alpha*(n-1)/n+1/n # finite-size correction
    }
    #if( n<50 && !finite && !nowarn){
    #  print("(plfit) Warning : finite-size bias may be present")
    #}

  }
  #
  #  end discrete case
  #
  ###########################################################################################

  #  return xmin, alpha and D in a list
  return(list(xmin=xmin,alpha=alpha,D=D))
}

library(VGAM)

###############################################################################
#
# library
#
# source the plfit.r function (from http://www.santafe.edu/~aaronc/powerlaws/)
# if needed
# source("plfit.r")
#
###############################################################################
#
# example zone
#
###############################################################################
runexample <- function(){
  set.seed(123)
  x <- (1-runif(1000))^(-1/(2.5-1))
  #if plfit exist (with source("plfit.r") for example):
  #plfit(x)
  #plfit R : xmin=1.007756 alpha=2.517830 D=0.02054889
  plpva(x,1)
  #plpva R p=0.233 gof=0.02164176
}
###############################################################################
# PLPVA  calculates the p-value for the given power-law fit to some data.
#    Source: http://www.santafe.edu/~aaronc/powerlaws/
#
#    PLPVA(x, xmin) takes data x and given lower cutoff for the power-law
#    behavior xmin and computes the corresponding p-value for the
#    Kolmogorov-Smirnov test, according to the method described in
#    Clauset, Shalizi, Newman (2007).
#    PLPVA automatically detects whether x is composed of real or integer
#    values, and applies the appropriate method. For discrete data, if
#    min(x) > 1000, PLPVA uses the continuous approximation, which is
#    a reliable in this regime.
#
#    The fitting procedure works as follows:
#    1) For each possible choice of x_min, we estimate alpha via the
#       method of maximum likelihood, and calculate the Kolmogorov-Smirnov
#       goodness-of-fit statistic D.
#    2) We then select as our estimate of x_min, the value that gives the
#       minimum value D over all values of x_min.
#
#    Note that this procedure gives no estimate of the uncertainty of the
#    fitted parameters, nor of the validity of the fit.
#
#    Example:
#       x <- (1-runif(10000))^(-1/(2.5-1))
#       plpva(x,1)
#
# Version 1.0   (2012 August)
#
# Copyright (C) 2012 Laurent Dubroca (Sete, France)
# laurent - dot -dubroca - at - gmail - dot - com
# Distributed under GPL 2.0
# http://www.gnu.org/copyleft/gpl.html
# PLPVA comes with ABSOLUTELY NO WARRANTY
# Matlab to R translation based on the original code of Aaron Clauset (Santa Fe Institute)
# Source: http://www.santafe.edu/~aaronc/powerlaws/
#
# Notes:
#
# 1. In order to implement the integer-based methods in R, the numeric
#    maximization of the log-likelihood function was used. This requires
#    that we specify the range of scaling parameters considered. We set
#    this range to be seq(1.5,3.5,0.01) by default. This vector can be
#    set by the user like so,
#
#       a <- plpva(x,1,vec=seq(1.001,5,0.001))
#
# 2. plvar can be told to limit the range of values considered as estimates
#    for xmin in two ways. First, it can be instructed to sample these
#    possible values like so,
#
#       a <- plpva(x,1,"sample",100)
#
#    which uses 100 uniformly distributed values on the sorted list of
#    unique values in the data set. Alternatively, it can simply omit all
#    candidates below a hard limit, like so
#
#       a <- plpva(x,1,"limit",3.4)
#
#    In the case of discrete data, it rounds the limit to the nearest
#    integer.
#
# 3. The default number of nonparametric repetitions of the fitting
# procedure is 1000. This number can be changed like so
#
#       a <- plpva(x,Bt=10000)
#
# 4. To silence the textual output to the screen, do
#
#       a <- plpva(x,1,quiet=TRUE)
#
###############################################################################
plpva<-function(x=rpareto(1000,10,2.5),xmin=1,method="limit",value=c(),Bt=500,quiet=TRUE,vec=seq(2.5, 7.5, .01)){
  #init method value to NULL
  sampl <- c() ; limit <- c()
  ###########################################################################################
  #
  #  test and trap for bad input
  #
  switch(method,
         sample = sampl <- value,
         limit = limit <- value,
         argok <- 0)

  if(exists("argok")){stop("(plvar) Unrecognized method")}

  if( !is.null(vec) && (!is.vector(vec) || min(vec)<=1 || length(vec)<=1) ){
    print(paste("(plvar) Error: ''range'' argument must contain a vector > 1; using default."))
    vec <- c()
  }
  if( !is.null(sampl) && ( !(sampl==floor(sampl)) ||  length(sampl)>1 || sampl<2 ) ){
    print(paste("(plvar) Error: ''sample'' argument must be a positive integer > 2; using default."))
    sample <- c()
  }
  if( !is.null(limit) && (length(limit)>1 || limit<1) ){
    print(paste("(plvar) Error: ''limit'' argument must be a positive >=1; using default."))
    limit <- c()
  }
  if( !is.null(Bt) && (!is.vector(Bt) || Bt<=1 || length(Bt)>1) ){
    print(paste("(plvar) Error: ''Bt'' argument must be a positive value > 1; using default."))
    vec <- c()
  }

  #  select method (discrete or continuous) for fitting and test if x is a vector
  fdattype<-"unknow"
  if( is.vector(x,"numeric") ){ fdattype<-"real" }
  if( all(x==floor(x)) && is.vector(x) ){ fdattype<-"integer" }
  if( all(x==floor(x)) && min(x) > 1000 && length(x) > 100 ){ fdattype <- "real" }
  if( fdattype=="unknow" ){ stop("(plfit) Error: x must contain only reals or only integers.") }

  N   <- length(x)
  nof <- rep(0,Bt)

  if( !quiet ){
    print("Power-law Distribution, parameter error calculation")
    print("Warning: This can be a slow calculation; please be patient.")
    print(paste(" n =",N,"xmin =",xmin,"- reps =",Bt,fdattype))
  }
  #
  #  end test and trap for bad input
  #
  ###########################################################################################

  ###########################################################################################
  #
  #  estimate xmin and alpha in the continuous case
  #
  if( fdattype=="real" ){

    # compute D for the empirical distribution
    z     <- x[x>=xmin];     nz   <- length(z)
    y     <- x[x<xmin];      ny   <- length(y)
    alpha <- 1 + nz/sum(log(z/xmin))
    cz    <- (0:(nz-1))/nz
    cf    <- 1-(xmin/sort(z))^(alpha-1)
    gof   <- max(abs(cz-cf))
    pz    <- nz/N

    # compute distribution of gofs from semi-parametric bootstrap
    # of entire data set with fit
    for(B in 1:length(nof)){
      # semi-parametric bootstrap of data
      n1 <- sum(runif(N)>pz)
      q1 <- y[sample(ny,n1,replace=TRUE)]
      n2 <- N-n1
      q2 <- xmin*(1-runif(n2))^(-1/(alpha-1))
      q  <- sort(c(q1,q2))

      # estimate xmin and alpha via GoF-method
      qmins <- sort(unique(q))
      qmins <- qmins[-length(qmins)]
      if(!is.null(limit)){
        qmins<-qmins[qmins<=limit]
      }
      if(!is.null(sampl)){
        qmins <- qmins[unique(round(seq(1,length(qmins),length.out=sampl)))]
      }
      dat <-rep(0,length(qmins))
      for(qm in 1:length(qmins)){
        qmin <- qmins[qm]
        zq   <- q[q>=qmin]
        nq   <- length(zq)
        a    <- nq/sum(log(zq/qmin))
        cq   <- (0:(nq-1))/nq
        cf   <- 1-(qmin/zq)^a
        dat[qm] <- max(abs(cq-cf))
      }
      if(!quiet){
        print(paste(B,sum(nof[1:B]>=gof)/B))
      }
      # store distribution of estimated gof values
      nof[B] <- min(dat)
    }
  }

  ###########################################################################################
  #
  #  estimate xmin and alpha in the discrete case
  #
  if( fdattype=="integer" ){

    if( is.null(vec) ){ vec<-seq(2.5, 7.5, .01) } # covers range of most practical scaling parameters
    zvec <- zeta(vec)

    # compute D for the empirical distribution
    z     <- x[x>=xmin];     nz   <- length(z);    xmax <- max(z)
    y     <- x[x<xmin];      ny   <- length(y)

    if(xmin==1){
      zdiff <- rep(0,length(vec))
    }else{
      zdiff <- apply(rep(t(1:(xmin-1)),length(vec))^-t(kronecker(t(array(1,xmin-1)),vec)),2,sum)
    }
    # matlab: L = -vec.*sum(log(z)) - n.*log(zvec - zdiff);
    L <- -vec*sum(log(z)) - nz*log(zvec - zdiff);
    I <- which.max(L)
    alpha <- vec[I]

    fit <- cumsum((((xmin:xmax)^-alpha))
                  / (zvec[I]
                     - (if (xmin == 1) 0 else sum((1:(xmin-1))^-alpha))))

    hist = aggregate(z, list(z), length)
    cdi = rep(0, xmax - xmin + 1)
    cdi[hist$Group.1 - xmin + 1] = hist$x / nz
    cdi = cumsum(cdi)

    gof <- max(abs( fit - cdi ))
    pz  <- nz/N

    mmax <- 20*xmax
    pdf <- c(rep(0,xmin-1),(((xmin:mmax)^-alpha))
             / (zvec[I]
                - (if (xmin == 1) 0 else sum((1:(xmin-1))^-alpha))))
    cdf <- cbind(1:(mmax+1),c(cumsum(pdf),1))

    # compute distribution of gofs from semi-parametric bootstrap
    # of entire data set with fit
    for(B in 1:Bt){
      # semi-parametric bootstrap of data
      n1 <- sum(runif(N)>pz)
      q1 <- y[sample(ny,n1,replace=TRUE)]
      n2 <- N-n1
      # simple discrete zeta generator
      r2 <- sort(runif(n2));  d <- 1
      q2 <- rep(0,n2);        k <- 1
      for(i in xmin:(mmax+1)){
        while((d <= length(r2)) && (r2[d]<=cdf[i,2])){d <- d+1}
        if (k <= d - 1)
          q2[k:(d-1)] <- i
        k <- d
        if( k > n2 ){break}
      }
      q <-c(q1,q2)
      ########################################
      # estimate xmin and alpha via GoF-method
      qmins <- sort(unique(q))
      qmins <- qmins[-length(qmins)]
      if(!is.null(limit)){
        qmins <- qmins[qmins<=limit]
      }
      if(!is.null(sampl)){
        qmins <- qmins[sort(unique(round(seq(1,length(qmins),length.out=sampl))))]
      }
      dat   <- rep(0,length(qmins))
      qmax  <- max(q)
      zq    <- q

      if (length(qmins) > 0)
        for(qm in 1:length(qmins)){
          qmin <- qmins[qm]
          zq   <- zq[zq>=qmin]
          nq    <- length(zq)
          if(nq>1){
            # vectorized version of numerical calculation
            if(qmin==1){
              #WARNING
              #zdiff <- rep(1,length(vec))
              #correction added the 6/10/2009 (following Naoki Masuda for plfit)
              zdiff <- rep(0,length(vec))
            }else{
              zdiff <- apply(rep(t(1:(qmin-1)),length(vec))^-t(kronecker(t(array(1,qmin-1)),vec)),2,sum)
            }
            L <- -vec*sum(log(zq)) - nq*log(zvec - zdiff);
            I <- which.max(L)
            # compute KS statistic
            fit <- cumsum((((qmin:qmax)^-vec[I]))
                          / (zvec[I]
                             - (if (qmin == 1) 0 else sum((1:(qmin-1))^-vec[I]))))

            hist = aggregate(zq, list(zq), length)
            cdi = rep(0, qmax - qmin + 1)
            cdi[hist$Group.1 - qmin + 1] = hist$x / nq
            cdi = cumsum(cdi)

            dat[qm] <- max(abs( fit - cdi ))
          }else{
            dat[qm] <- -Inf
          }
        }
      if(!quiet){
        print(paste(B,"-",sum(nof[1:B]>=gof)/B))
      }
      # -- store distribution of estimated gof values
      nof[B] <- min(c(dat, Inf))


    }

    ####################################

  }
  #
  #  end discrete case
  #
  ###########################################################################################
  #  return p and gof in a list
  p <- sum(nof>=gof)/length(nof)
  return(list(p=p,gof=gof))
}
###############################################################################
gof_trad_pldis <- function(x, xmin, alpha, Bt = 500) {
  #init method value to NULL
  sampl <- c()
  limit <- c()
  N   <- length(x)
  nof <- rep(0, Bt)

  vec <- seq(1.1, 7.5, .01)
  zvec <- zeta(vec)

  # compute D for the empirical distribution
  z <- x[x >= xmin]; nz <- length(z); xmax <- max(z)
  y <- x[x < xmin];  ny <- length(y)

  fit <- cumsum(
    ((xmin:xmax)^-alpha)/(zeta(alpha) - (if (xmin == 1) 0 else sum((1:(xmin - 1))^-alpha)))
  )

  hist <- aggregate(z, list(z), length)
  cdi <- rep(0, xmax - xmin + 1)
  cdi[hist$Group.1 - xmin + 1] <- hist$x / nz
  cdi <- cumsum(cdi)

  gof <- max(abs(fit - cdi))
  pz  <- nz/N

  mmax <- 20 * xmax
  pdf <- c(rep(0, xmin - 1),
           ((xmin:mmax)^-alpha)/(zeta(alpha) - (if (xmin == 1) 0 else sum((1:(xmin - 1))^-alpha)))
  )
  cdf <- cbind(1:(mmax + 1), c(cumsum(pdf), 1))

  # compute distribution of gofs from semi-parametric bootstrap
  # of entire data set with fit
  for (B in 1:Bt) {
    # semi-parametric bootstrap of data
    n1 <- sum(runif(N) > pz)
    q1 <- y[sample(ny, n1, replace = TRUE)]
    n2 <- N - n1
    # simple discrete zeta generator
    r2 <- sort(runif(n2));  d <- 1
    q2 <- rep(0, n2);        k <- 1
    for (i in xmin:(mmax + 1)) {
      while ((d <= length(r2)) && (r2[d] <= cdf[i, 2])) { d <- d + 1 }
      if (k <= d - 1) q2[k:(d - 1)] <- i
      k <- d
      if (k > n2) { break }
    }
    q <- c(q1, q2)
    ########################################
    # estimate xmin and alpha via GoF-method
    qmins <- sort(unique(q))
    qmins <- qmins[-length(qmins)]
    if (!is.null(limit)) {
      qmins <- qmins[qmins <= limit]
    }
    if (!is.null(sampl)) {
      qmins <- qmins[sort(unique(round(seq(1,length(qmins),length.out=sampl))))]
    }
    dat   <- rep(0, length(qmins))
    qmax  <- max(q)
    zq    <- q

    if (length(qmins) > 0)
      for (qm in 1:length(qmins)) {
        qmin <- qmins[qm]
        zq   <- zq[zq >= qmin]
        nq   <- length(zq)
        if (nq > 1) {
          if (qmin == 1) {
            zdiff <- rep(0, length(vec))
          }else{
            zdiff <- apply(
              rep(t(1:(qmin - 1)), length(vec))^-t(kronecker(t(array(1, qmin - 1)), vec)), 2, sum)
          }
          L <- -vec * sum(log(zq)) - nq * log(zvec - zdiff)
          I <- which.max(L)

          # compute KS statistic
          fit <- cumsum(
            ((qmin:qmax)^-vec[I])/(zvec[I] - (if (qmin == 1) 0 else sum((1:(qmin-1))^-vec[I])))
          )

          hist <- aggregate(zq, list(zq), length)
          cdi <- rep(0, qmax - qmin + 1)
          cdi[hist$Group.1 - qmin + 1] <- hist$x / nq
          cdi <- cumsum(cdi)

          dat[qm] <- max(abs(fit - cdi))
        }else{
          dat[qm] <- -Inf
        }
      }
    nof[B] <- min(c(dat, Inf))
  }

  p <- mean(nof >= gof)
  return(list(p = p, gof = gof))
}
